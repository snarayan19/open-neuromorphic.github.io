<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="SNN training frameworks for ML Open-source software to train spiking neural networks for ML tasks.
snnTorch snnTorch is a SNN training framework for machine learning applications. It is focused on gradient-based training of SNNs. It is based on PyTorch for GPU acceleration and gradient computation.
Norse Norse aims at exploiting the advantages of bio-inspired neural components, which are sparse and event-driven - a fundamental difference from artificial neural networks. Norse expands PyTorch with primitives for bio-inspired neural components, bringing you two advantages: a modern and proven infrastructure based on PyTorch and deep learning-compatible spiking neural network components."><title>Resources</title><link rel=canonical href=https://open-neuromorphic.github.io/resources/><link rel=stylesheet href=/scss/style.min.8191399262444ab68b72a18c97392f5349be20a1615d77445be51e974c144cff.css><meta property="og:title" content="Resources"><meta property="og:description" content="SNN training frameworks for ML Open-source software to train spiking neural networks for ML tasks.
snnTorch snnTorch is a SNN training framework for machine learning applications. It is focused on gradient-based training of SNNs. It is based on PyTorch for GPU acceleration and gradient computation.
Norse Norse aims at exploiting the advantages of bio-inspired neural components, which are sparse and event-driven - a fundamental difference from artificial neural networks. Norse expands PyTorch with primitives for bio-inspired neural components, bringing you two advantages: a modern and proven infrastructure based on PyTorch and deep learning-compatible spiking neural network components."><meta property="og:url" content="https://open-neuromorphic.github.io/resources/"><meta property="og:site_name" content="Open Neuromorphic"><meta property="og:type" content="article"><meta property="article:section" content="Page"><meta name=twitter:title content="Resources"><meta name=twitter:description content="SNN training frameworks for ML Open-source software to train spiking neural networks for ML tasks.
snnTorch snnTorch is a SNN training framework for machine learning applications. It is focused on gradient-based training of SNNs. It is based on PyTorch for GPU acceleration and gradient computation.
Norse Norse aims at exploiting the advantages of bio-inspired neural components, which are sparse and event-driven - a fundamental difference from artificial neural networks. Norse expands PyTorch with primitives for bio-inspired neural components, bringing you two advantages: a modern and proven infrastructure based on PyTorch and deep learning-compatible spiking neural network components."><link rel="shortcut icon" href=/img/ONM-logo.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/ONM-logo.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Open Neuromorphic</a></h1><h2 class=site-description>Organization that aims at providing one place to reference all relevant open-source project in the neuromorphic research domain.</h2></div></header><ol class=social-menu><li><a href=https://discord.gg/C9bzWgNmqk target=_blank title=Discord rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-discord" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="9" cy="12" r="1"/><circle cx="15" cy="12" r="1"/><path d="M7.5 7.5c3.5-1 5.5-1 9 0"/><path d="M7 16.5c3.5 1 6.5 1 10 0"/><path d="M15.5 17c0 1 1.5 3 2 3 1.5.0 2.833-1.667 3.5-3 .667-1.667.5-5.833-1.5-11.5-1.457-1.015-3-1.34-4.5-1.5l-1 2.5"/><path d="M8.5 17c0 1-1.356 3-1.832 3-1.429.0-2.698-1.667-3.333-3-.635-1.667-.476-5.833 1.428-11.5C6.151 4.485 7.545 4.16 9 4l1 2.5"/></svg></a></li><li><a href=https://github.com/open-neuromorphic target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/groups/9267873 target=_blank title=LinkedIn rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="4" width="16" height="16" rx="2"/><line x1="8" y1="11" x2="8" y2="16"/><line x1="8" y1="8" x2="8" y2="8.01"/><line x1="12" y1="16" x2="12" y2="11"/><path d="M16 16v-3a2 2 0 00-4 0"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/events/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-event" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="5" width="16" height="16" rx="2"/><line x1="16" y1="3" x2="16" y2="7"/><line x1="8" y1="3" x2="8" y2="7"/><line x1="4" y1="11" x2="20" y2="11"/><rect x="8" y="15" width="2" height="2"/></svg><span>Events</span></a></li><li><a href=/opportunities/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-bulb" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 12h1m8-9v1m8 8h1M5.6 5.6l.7.7m12.1-.7-.7.7"/><path d="M9 16a5 5 0 116 0 3.5 3.5.0 00-1 3 2 2 0 01-4 0 3.5 3.5.0 00-1-3"/><line x1="9.7" y1="17" x2="14.3" y2="17"/></svg><span>Opportunities</span></a></li><li class=current><a href=/resources/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-code" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><polyline points="7 8 3 12 7 16"/><polyline points="17 8 21 12 17 16"/><line x1="14" y1="4" x2="10" y2="20"/></svg><span>Resources</span></a></li><li><a href=/team/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-users" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="9" cy="7" r="4"/><path d="M3 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/><path d="M16 3.13a4 4 0 010 7.75"/><path d="M21 21v-2a4 4 0 00-3-3.85"/></svg><span>Team</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-open-source" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 3a9 9 0 013.618 17.243l-2.193-5.602a3 3 0 10-2.849.0l-2.193 5.603A9 9 0 0112 3z"/></svg><span>About</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#snn-training-frameworks-for-ml>SNN training frameworks for ML</a><ol><li><a href=#snntorch>snnTorch</a></li><li><a href=#norse>Norse</a></li><li><a href=#sinabs>Sinabs</a></li><li><a href=#rockpool>Rockpool</a></li><li><a href=#bindsnet>BindsNET</a></li></ol></li><li><a href=#snn-training-frameworks-for-neuroscience>SNN training frameworks for neuroscience</a><ol><li><a href=#brian>Brian</a></li><li><a href=#nest>NEST</a></li></ol></li><li><a href=#snn-training-frameworks-for-ml-and-neuroscience>SNN training frameworks for ML and neuroscience</a><ol><li><a href=#nengo>Nengo</a></li><li><a href=#lava>Lava</a></li></ol></li><li><a href=#event-based-data-utilities>Event-based data utilities</a><ol><li><a href=#tonic>Tonic</a></li><li><a href=#expelliarmus>Expelliarmus</a></li><li><a href=#aestream---address-event-streaming-library>AEStream - Address Event Streaming library</a></li><li><a href=#aedat>AEDAT</a></li><li><a href=#bimvee>Bimvee</a></li><li><a href=#mustard>MUSTARD</a></li></ol></li><li><a href=#digital-hardware-projects>Digital hardware projects</a><ol><li><a href=#odin-spiking-neural-network-snn-processor>ODIN Spiking Neural Network (SNN) Processor</a></li><li><a href=#reckon-a-spiking-rnn-processor-enabling-on-chip-learning-over-second-long-timescales>ReckOn: A Spiking RNN Processor Enabling On-Chip Learning over Second-Long Timescales</a></li><li><a href=#ranc---reconfigurable-architecture-for-neuromorphic-computing>RANC - Reconfigurable Architecture for Neuromorphic Computing</a></li><li><a href=#sne-an-energy-proportional-digital-accelerator-for-sparse-event-based-convolutions>SNE: an Energy-Proportional Digital Accelerator for Sparse Event-Based Convolutions</a></li><li><a href=#a-lightweight-spiking-neural-network-accelerator-google-shuttle>A Lightweight Spiking Neural Network Accelerator [Google Shuttle]</a></li><li><a href=#snn-asic-accelerator-google-shuttle>SNN ASIC accelerator [Google Shuttle]</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/resources/>Resources</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>7 minute read</time></div></footer></div></header><section class=article-content><h2 id=snn-training-frameworks-for-ml>SNN training frameworks for ML</h2><p>Open-source software to train spiking neural networks for ML tasks.</p><h3 id=snntorch>snnTorch</h3><p><img src=/resources/snntorch.png width=500 height=98 srcset="/resources/snntorch_huf74f01e7628f54407d08fbb235a720cb_7946_480x0_resize_box_3.png 480w, /resources/snntorch_huf74f01e7628f54407d08fbb235a720cb_7946_1024x0_resize_box_3.png 1024w" loading=lazy alt=snnTorch class=gallery-image data-flex-grow=510 data-flex-basis=1224px></p><p><a class=link href=https://github.com/jeshraghian/snntorch target=_blank rel=noopener>snnTorch</a> is a SNN training framework for machine learning applications. It is focused on gradient-based training of SNNs.
It is based on PyTorch for GPU acceleration and gradient computation.</p><h3 id=norse>Norse</h3><p><img src=/resources/norse.png width=500 height=165 srcset="/resources/norse_hu7dbb30bd36b28e17e74da23c111d32f2_15941_480x0_resize_box_3.png 480w, /resources/norse_hu7dbb30bd36b28e17e74da23c111d32f2_15941_1024x0_resize_box_3.png 1024w" loading=lazy alt=Norse class=gallery-image data-flex-grow=303 data-flex-basis=727px></p><p><a class=link href=https://norse.github.io/norse/ target=_blank rel=noopener>Norse</a> aims at exploiting the advantages of bio-inspired neural components, which are sparse and event-driven - a fundamental difference from artificial neural networks. Norse expands PyTorch with primitives for bio-inspired neural components, bringing you two advantages: a modern and proven infrastructure based on PyTorch and deep learning-compatible spiking neural network components.</p><h3 id=sinabs>Sinabs</h3><p><img src=/resources/sinabs.png width=500 height=158 srcset="/resources/sinabs_hufcf785e170fb2ae7c688c11495f36592_18291_480x0_resize_box_3.png 480w, /resources/sinabs_hufcf785e170fb2ae7c688c11495f36592_18291_1024x0_resize_box_3.png 1024w" loading=lazy alt=Sinabs class=gallery-image data-flex-grow=316 data-flex-basis=759px></p><p><a class=link href=https://sinabs.readthedocs.io target=_blank rel=noopener>Sinabs</a> is a deep learning library based on PyTorch for spiking neural networks, with a focus on simplicity, fast training and extendability. Sinabs works well for Vision models because of its support for weight transfer.</p><h3 id=rockpool>Rockpool</h3><p><img src=/resources/rockpool.png width=500 height=442 srcset="/resources/rockpool_hu68a584d47ad93d57f68a37c61ae13b56_36044_480x0_resize_box_3.png 480w, /resources/rockpool_hu68a584d47ad93d57f68a37c61ae13b56_36044_1024x0_resize_box_3.png 1024w" loading=lazy alt=Rockpool class=gallery-image data-flex-grow=113 data-flex-basis=271px></p><p><a class=link href=https://synsense.gitlab.io/rockpool/ target=_blank rel=noopener>Rockpool</a> is a Python package for working with dynamical neural network architectures, particularly for designing event-driven networks for Neuromorphic computing hardware. Rockpool provides a convenient interface for designing, training and evaluating recurrent networks, which can operate both with continuous-time dynamics and event-driven dynamics.</p><h3 id=bindsnet>BindsNET</h3><p><img src=/resources/bindsnet.jpg width=308 height=131 srcset="/resources/bindsnet_hu382e57018da975de3df0944cbafd0a92_5086_480x0_resize_q75_box.jpg 480w, /resources/bindsnet_hu382e57018da975de3df0944cbafd0a92_5086_1024x0_resize_q75_box.jpg 1024w" loading=lazy alt=BindsNET class=gallery-image data-flex-grow=235 data-flex-basis=564px></p><p><a class=link href=https://bindsnet-docs.readthedocs.io/ target=_blank rel=noopener>BindsNET</a> is built on top of the PyTorch deep learning platform. It is used for the simulation of spiking neural networks (SNNs) and is geared towards machine learning and reinforcement learning.</p><h2 id=snn-training-frameworks-for-neuroscience>SNN training frameworks for neuroscience</h2><p>Open-source software to train spiking neural networks. In these the tools, the goal is to emulate the human brain as efficiently as possible, instead of optimizing neuron model and simulation for speeding up AI tasks.</p><h3 id=brian>Brian</h3><p><img src=/resources/brian.png width=500 height=264 srcset="/resources/brian_hu5ad225ea7651a0dfbcb2f550fb6da5a0_87184_480x0_resize_box_3.png 480w, /resources/brian_hu5ad225ea7651a0dfbcb2f550fb6da5a0_87184_1024x0_resize_box_3.png 1024w" loading=lazy alt=Brian class=gallery-image data-flex-grow=189 data-flex-basis=454px></p><p><a class=link href=https://briansimulator.org/ target=_blank rel=noopener>Brian</a> is a free, open source simulator for spiking neural networks. It is written in the Python programming language and is available on almost all platforms. We believe that a simulator should not only save the time of processors, but also the time of scientists. Brian is therefore designed to be easy to learn and use, highly flexible and easily extensible.</p><h3 id=nest>NEST</h3><p><img src=/resources/nest.png width=500 height=208 srcset="/resources/nest_hue58ddd7f6dad89171a00065240db3d90_16239_480x0_resize_box_3.png 480w, /resources/nest_hue58ddd7f6dad89171a00065240db3d90_16239_1024x0_resize_box_3.png 1024w" loading=lazy alt=NEST class=gallery-image data-flex-grow=240 data-flex-basis=576px></p><p><a class=link href=https://www.nest-simulator.org/ target=_blank rel=noopener>NEST</a> is a simulator for spiking neural network models that focuses on the dynamics, size and structure of neural systems rather than on the exact morphology of individual neurons. The development of NEST is coordinated by the NEST Initiative.</p><p>NEST is ideal for networks of spiking neurons of any size, for example:</p><ul><li>models of information processing e.g. in the visual or auditory cortex of mammals,</li><li>models of network activity dynamics, e.g. laminar cortical networks or balanced random networks,</li><li>models of learning and plasticity.</li></ul><h2 id=snn-training-frameworks-for-ml-and-neuroscience>SNN training frameworks for ML and neuroscience</h2><p>Open-source software to train spiking neural networks for both neuroscience and ML applications.</p><h3 id=nengo>Nengo</h3><p><img src=/resources/nengo.jpg width=500 height=281 srcset="/resources/nengo_hu4805065fd892aabca50b2167f9cf5526_21873_480x0_resize_q75_box.jpg 480w, /resources/nengo_hu4805065fd892aabca50b2167f9cf5526_21873_1024x0_resize_q75_box.jpg 1024w" loading=lazy alt=Nengo class=gallery-image data-flex-grow=177 data-flex-basis=427px></p><p><a class=link href=https://nengo.ai target=_blank rel=noopener>Nengo</a> is a Python package for building, testing, and deploying neural networks. It supports plenty of backends for the SNN simulation.</p><h3 id=lava>Lava</h3><p><img src=/resources/lava.png width=500 height=91 srcset="/resources/lava_hudb73700554a80277dfb9408bfce0bb1c_30882_480x0_resize_box_3.png 480w, /resources/lava_hudb73700554a80277dfb9408bfce0bb1c_30882_1024x0_resize_box_3.png 1024w" loading=lazy alt=Lava class=gallery-image data-flex-grow=549 data-flex-basis=1318px></p><p><a class=link href=https://lava-nc.org/ target=_blank rel=noopener>Lava</a> is an open-source software framework for developing neuro-inspired applications and mapping them to neuromorphic hardware. Lava provides developers with the tools and abstractions to develop applications that fully exploit the principles of neural computation. Constrained in this way, like the brain, Lava applications allow neuromorphic platforms to intelligently process, learn from, and respond to real-world data with great gains in energy efficiency and
speed compared to conventional computer architectures.</p><h2 id=event-based-data-utilities>Event-based data utilities</h2><p>Open-source software to handle event-based data, including data generated by dynamic vision sensors and other neuromorphic sensors.</p><h3 id=tonic>Tonic</h3><p><img src=/resources/tonic.png width=500 height=107 srcset="/resources/tonic_hu3faf3561bfaadcaea6d335360f222ecb_10612_480x0_resize_box_3.png 480w, /resources/tonic_hu3faf3561bfaadcaea6d335360f222ecb_10612_1024x0_resize_box_3.png 1024w" loading=lazy alt=Tonic class=gallery-image data-flex-grow=467 data-flex-basis=1121px></p><p><a class=link href=https://tonic.readthedocs.io target=_blank rel=noopener>Tonic</a> is a tool to facilitate the download, manipulation and loading of event-based/spike-based data. It&rsquo;s like PyTorch Vision but for neuromorphic data!</p><h3 id=expelliarmus>Expelliarmus</h3><p><img src=/resources/expelliarmus.png width=500 height=177 srcset="/resources/expelliarmus_hua75d7a1693abe3c52f49f922413ed7a0_40266_480x0_resize_box_3.png 480w, /resources/expelliarmus_hua75d7a1693abe3c52f49f922413ed7a0_40266_1024x0_resize_box_3.png 1024w" loading=lazy alt=Expelliarmus class=gallery-image data-flex-grow=282 data-flex-basis=677px></p><p><a class=link href=https://expelliarmus.readthedocs.io target=_blank rel=noopener>Expelliarmus</a> is a Python/C package to decode binary files produced by Prophesee cameras.</p><h3 id=aestream---address-event-streaming-library>AEStream - Address Event Streaming library</h3><p><a class=link href=https://github.com/norse/aestream target=_blank rel=noopener>AEStream</a> parses event-based dynamic-vision system (DVS) data from an input source and streams it to a sink (GPU, CPU, network ports&mldr;).</p><h3 id=aedat>AEDAT</h3><p><a class=link href=https://github.com/open-neuromorphic/aedat target=_blank rel=noopener>AEDAT</a> is a fast AEDAT 4 python reader, with a Rust underlying implementation.</p><h3 id=bimvee>Bimvee</h3><p><img src=/resources/bimvee.png width=500 height=449 srcset="/resources/bimvee_hu9da9dc174d7385e2ed93f507d49b384d_137744_480x0_resize_box_3.png 480w, /resources/bimvee_hu9da9dc174d7385e2ed93f507d49b384d_137744_1024x0_resize_box_3.png 1024w" loading=lazy alt=Bimvee class=gallery-image data-flex-grow=111 data-flex-basis=267px></p><p><a class=link href=https://github.com/event-driven-robotics/bimvee target=_blank rel=noopener>BIMVEE</a> is an open-source python library for Batch Import, Manipulation, Visualisation and Export of Events etc. It has import routines for several different event formats including, notably, a python-native unpacker of rpg-rosbags (no ROS installation required). It imports event data together with bundled data such as rgb images, point-clouds, IMU etc, handling timestamp synchronisation. It has visualisers with
intuitive defaults, allowing quick visualisation of the contents of event-data containers, and it has export routines for several data formats. Manipulations include time-stamp re-alignments, spatial cropping and event filtering.</p><h3 id=mustard>MUSTARD</h3><p><img src=/resources/mustard.png width=500 height=270 srcset="/resources/mustard_huc3220ea43b7b644d38f8ce97e0e89e45_50588_480x0_resize_box_3.png 480w, /resources/mustard_huc3220ea43b7b644d38f8ce97e0e89e45_50588_1024x0_resize_box_3.png 1024w" loading=lazy alt=MUSTARD class=gallery-image data-flex-grow=185 data-flex-basis=444px></p><p><a class=link href=https://github.com/event-driven-robotics/mustard target=_blank rel=noopener>MUSTARD</a> is an open-source python application for playback of multistream data, including event-camera data. Each data stream present in a container (such as rosbag, but it can import any data-type which BIMVEE supports) opens a visualiser sub-window and data playback of all visualisers is controlled globally, including the speed, direction and time-window of visualisation. It has visualisers for events, rgb images, 6-DOF poses,
point-clouds, depth maps etc.</p><h2 id=digital-hardware-projects>Digital hardware projects</h2><p>Open-source digital hardware projects.</p><h3 id=odin-spiking-neural-network-snn-processor>ODIN Spiking Neural Network (SNN) Processor</h3><p><img src=/resources/odin-frenkel.png width=500 height=505 srcset="/resources/odin-frenkel_hu4fa4b92c38c085754bc573d5eee8a853_454879_480x0_resize_box_3.png 480w, /resources/odin-frenkel_hu4fa4b92c38c085754bc573d5eee8a853_454879_1024x0_resize_box_3.png 1024w" loading=lazy alt=ODIN class=gallery-image data-flex-grow=99 data-flex-basis=237px></p><p><a class=link href=https://github.com/ChFrenkel/ODIN target=_blank rel=noopener>ODIN</a> is an online-learning digital spiking neuromorphic processor designed and prototyped in 28-nm FDSOI CMOS at Université catholique de Louvain (UCLouvain), published in 2019 in the IEEE Transactions on Biomedical Circuits and Systems journal. ODIN is based on a single 256-neuron 64k-synapse crossbar neurosynaptic core with the following key features:</p><ul><li>synapses embed spike-dependent synaptic plasticity (SDSP)-based online learning,</li><li>neurons can phenomenologically reproduce the 20 Izhikevich behaviors.</li></ul><p>ODIN is thus a versatile experimentation platform for learning at the edge, while demonstrating (i) record neuron and synapse densities compared to all previously-proposed spiking neural networks (SNNs) and (ii) the lowest energy per synaptic operation across previously-proposed digital SNNs.</p><h3 id=reckon-a-spiking-rnn-processor-enabling-on-chip-learning-over-second-long-timescales>ReckOn: A Spiking RNN Processor Enabling On-Chip Learning over Second-Long Timescales</h3><p><img src=/resources/reckon-frenkel.png width=500 height=484 srcset="/resources/reckon-frenkel_huf34fffb504d01b45e95e2f51361107de_496226_480x0_resize_box_3.png 480w, /resources/reckon-frenkel_huf34fffb504d01b45e95e2f51361107de_496226_1024x0_resize_box_3.png 1024w" loading=lazy alt=ReckOn class=gallery-image data-flex-grow=103 data-flex-basis=247px></p><p><a class=link href=https://github.com/ChFrenkel/ReckOn target=_blank rel=noopener>ReckOn</a> is a spiking recurrent neural network (RNN) processor enabling on-chip learning over second-long timescales based on a modified version of the e-prop algorithm (we released a PyTorch implementation of the vanilla e-prop algorithm for leaky integrate-and-fire neurons here). It was prototyped and measured in 28-nm FDSOI CMOS at the Institute of Neuroinformatics, University of Zurich and ETH Zurich, and published at the 2022 IEEE
International Solid-State Circuits Conference (ISSCC) with the following three main claims:</p><ul><li>ReckOn demonstrates end-to-end on-chip learning over second-long timescales while keeping a milli-second temporal resolution,</li><li>it provides a low-cost solution with a 0.45-mm² core area, 5.3pJ/SOP at 0.5V, and a memory overhead of only 0.8% compared to the equivalent inference-only network,</li><li>it exploits a spike-based representation for task-agnostic learning toward user customization and chip repurposing at the edge.</li></ul><h3 id=ranc---reconfigurable-architecture-for-neuromorphic-computing>RANC - Reconfigurable Architecture for Neuromorphic Computing</h3><p><img src=/resources/ranc.png width=500 height=313 srcset="/resources/ranc_hu2c0deffe0c6b144e028a2645de8aab6a_44572_480x0_resize_box_3.png 480w, /resources/ranc_hu2c0deffe0c6b144e028a2645de8aab6a_44572_1024x0_resize_box_3.png 1024w" loading=lazy alt=RANC class=gallery-image data-flex-grow=159 data-flex-basis=383px></p><p><a class=link href=https://ua-rcl.github.io/RANC/ target=_blank rel=noopener>RANC</a> is a highly flexible environment that enables rapid experimentation with neuromorphic architectures in both software via C++ simulation and hardware via FPGA emulation. RANC enables hardware architects and application engineers to investigate and tune parameters of their neuromorphic architecture that would otherwise be unavailable on a purely prefabricated ASIC. This level of flexibility creates an environment that allows for optimizing
architectures based on application insights as well as prototyping future neuromorphic architectures that can support new classes of applications entirely.</p><h3 id=sne-an-energy-proportional-digital-accelerator-for-sparse-event-based-convolutions>SNE: an Energy-Proportional Digital Accelerator for Sparse Event-Based Convolutions</h3><p><img src=/resources/sne-di-mauro.png width=500 height=383 srcset="/resources/sne-di-mauro_hua2ed77dd49964b2aed676c22a438f59f_135809_480x0_resize_box_3.png 480w, /resources/sne-di-mauro_hua2ed77dd49964b2aed676c22a438f59f_135809_1024x0_resize_box_3.png 1024w" loading=lazy alt=SNE class=gallery-image data-flex-grow=130 data-flex-basis=313px></p><p><a class=link href=https://github.com/pulp-platform/sne target=_blank rel=noopener>SNE</a> is a novel digital sparse neural engine (SNE) to efficiently accelerate SNN inference tasks at the extreme edge. The accelerator exploits an explicit input event temporal and spatial location encoding; the SNE architecture is designed to improve input data and weight reuse, reducing the traffic towards the memory. SNE achieves a maximum performance of 51.2 GSOP/s, and an energy efficiency of 4.5TSOP/s/W. Ultimately, SNE shows 3.55X higher
energy efficiency than SoA neuromorphic platform [16], approaching classical DNN accelerators energy efficiencies, while performing energy-proportional computations. As a proof of concept, it is shown that SNE consumes 0.221 pJ/SOP and achieves 92.8% accuracy on a classification task performed on the IBM DVS-Gesture data set.</p><h3 id=a-lightweight-spiking-neural-network-accelerator-google-shuttle>A Lightweight Spiking Neural Network Accelerator [Google Shuttle]</h3><p><img src=/resources/snn-asic-google-jason.png width=500 height=418 srcset="/resources/snn-asic-google-jason_hucdcbcf4fc964e39fd2092f424e73dde9_502842_480x0_resize_box_3.png 480w, /resources/snn-asic-google-jason_hucdcbcf4fc964e39fd2092f424e73dde9_502842_1024x0_resize_box_3.png 1024w" loading=lazy alt="A Lightweight Spiking Neural Network Accelerator" class=gallery-image data-flex-grow=119 data-flex-basis=287px></p><p><a class=link href=https://github.com/jeshraghian/snn-accelerator target=_blank rel=noopener>A spiking neural network accelerator</a> in the SkyWater 130nm process using heterogenous time constants to model a variety of temporal dynamics. Input events can be streamed at a rate of up to 50 MEvents per second, and the accelerator can process them in a dense network with 128 hidden neurons at up to approximately 214 MHz.</p><h3 id=snn-asic-accelerator-google-shuttle>SNN ASIC accelerator [Google Shuttle]</h3><p><img src=/resources/snn-asic-google-pengzhou.png width=500 height=530 srcset="/resources/snn-asic-google-pengzhou_hu49f65a5221695d3c75555f7fa4dd038e_250696_480x0_resize_box_3.png 480w, /resources/snn-asic-google-pengzhou_hu49f65a5221695d3c75555f7fa4dd038e_250696_1024x0_resize_box_3.png 1024w" loading=lazy alt="SNN ASIC accelerator" class=gallery-image data-flex-grow=94 data-flex-basis=226px></p><p>An <a class=link href=https://github.com/pengzhouzp/wrapped_snn_network target=_blank rel=noopener>SNN ASIC</a> with adaptive threshold neurons and recurrent connective synapses.</p></section><footer class=article-footer></footer></article><div class="article-list--compact links"><article><a href=https://github.com/open-neuromorphic/open-neuromorphic target=_blank rel=noopener><div class=article-details><h2 class=article-title>Open Neuromorphic GitHub repository</h2><footer class=article-time>Repository listing the resources.</footer></div><div class=article-image><img src=https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png loading=lazy></div></a></article></div><footer class=site-footer><section class=copyright>&copy;
2022 -
2023 Open Neuromorphic</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.16.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>